<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Universal Speech ↔ Text Demo</title>
  <style>
    body { font-family: sans-serif; padding: 1rem; max-width: 600px; margin: auto; }
    #log { white-space: pre-wrap; border: 1px solid #ccc; padding: 1rem; height: 200px; overflow: auto; margin-top: 1rem; }
    button { padding: .5rem 1rem; font-size: 1rem; margin-right: .5rem; }
    textarea { width: 100%; height: 4em; margin-top: .5rem; }
    .controls { margin-top: 1rem; }
  </style>
</head>
<body>

  <h1>Speech ↔ Text Demo</h1>

  <!-- Text‑to‑Speech -->
  <section>
    <h2>Text → Speech</h2>
    <textarea id="ttsInput" placeholder="Type text here…"></textarea>
    <div class="controls">
      <button id="speakBtn">Speak Text</button>
    </div>
  </section>

  <!-- Speech‑to‑Text -->
  <section>
    <h2>Speech → Text</h2>
    <div class="controls">
      <button id="startBtn">Start Recognition</button>
      <button id="stopBtn" disabled>Stop Recognition</button>
    </div>
    <div id="log">Click “Start Recognition” to begin…</div>
  </section>

  <!-- Vosk‑Browser (WASM) -->
  <script src="https://cdn.jsdelivr.net/npm/vosk-browser@0.0.8/dist/vosk.js"></script>
  <script>
    // --- Text-to-Speech ---
    document.getElementById('speakBtn').onclick = () => {
      const text = document.getElementById('ttsInput').value.trim();
      if (!text) return;
      const utter = new SpeechSynthesisUtterance(text);
      speechSynthesis.speak(utter);
    };

    // --- Speech-to-Text Setup ---
    const logEl = document.getElementById('log');
    const log = msg => { logEl.textContent += msg + '\n'; };

    const NativeRecog = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recog, voskRec, voskProcessor, voskAudioCtx;

    document.getElementById('startBtn').onclick = async () => {
      logEl.textContent = '';               // clear previous logs
      document.getElementById('stopBtn').disabled = false;
      document.getElementById('startBtn').disabled = true;

      if (NativeRecog) {
        startNative();
      } else {
        await startVosk();
      }
    };

    document.getElementById('stopBtn').onclick = () => {
      document.getElementById('stopBtn').disabled = true;
      document.getElementById('startBtn').disabled = false;

      if (NativeRecog) {
        recog.stop();
      } else if (voskProcessor) {
        voskProcessor.disconnect();
        voskAudioCtx.close();
        log('⏹ Vosk recognition stopped');
      }
    };

    // 1) Native Web Speech API branch
    function startNative() {
      log('▶ Using native SpeechRecognition API');
      recog = new NativeRecog();
      recog.lang = 'en-US';
      recog.interimResults = false;   // only final results
      recog.maxAlternatives = 1;

      recog.onresult = ev => {
        const text = ev.results[0][0].transcript;
        log('🎤 Transcribed: ' + text);
      };
      recog.onerror = ev => log('⚠️ Error: ' + ev.error);
      recog.onend = () => log('⏹ Native recognition ended');

      recog.start();
    }

    // 2) Vosk‑WASM fallback branch
    async function startVosk() {
      log('▶ Native API unavailable → loading Vosk fallback…');
      const model = await Vosk.createModel('model.tar.gz');
      log('✔ Vosk model loaded');

      const rec = new model.KaldiRecognizer({ sampleRate: 16000 });
      rec.on('result', ({ result }) => {
        log('🎤 Transcribed: ' + result.text);
      });
      rec.on('partialresult', ({ result }) => {
        log('…partial: ' + result.partial);
      });
      voskRec = rec;

      // audio chain
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { echoCancellation: true, noiseSuppression: true, channelCount: 1, sampleRate: 16000 }
      });
      log('✔ Microphone access granted');

      voskAudioCtx = new AudioContext();
      const source = voskAudioCtx.createMediaStreamSource(stream);
      voskProcessor = voskAudioCtx.createScriptProcessor(4096, 1, 1);

      source.connect(voskProcessor);
      voskProcessor.connect(voskAudioCtx.destination);

      voskProcessor.onaudioprocess = e => {
        const data = e.inputBuffer.getChannelData(0);
        const int16 = new Int16Array(data.length);
        for (let i = 0; i < data.length; ++i) {
          int16[i] = data[i] * 0x7FFF;
        }
        voskRec.acceptWaveform(int16);
      };
    }
  </script>
</body>
</html>
